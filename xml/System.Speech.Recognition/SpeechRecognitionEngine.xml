<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="713dc25516fc1274a2b0fdeb1e88295f3064fd9a" />
    <Meta Name="ms.sourcegitcommit" Value="6a0b904069161bbaec4ffd02aa7d9cf38c61e72e" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="ja-JP" />
    <Meta Name="ms.lasthandoff" Value="06/24/2018" />
    <Meta Name="ms.locfileid" Value="36409733" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>インプロセス音声認識エンジンにアクセスしてエンジンを管理するための手段を提供します。</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 このクラスのインスタンスを作成するにはインストールされている音声認識機能のいずれか。 認識エンジンのインストールに関する情報を取得するには、使用、静的な<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>メソッドです。  
  
 このクラスは、音声認識エンジン、インプロセスで実行されているは、し、次のように、音声認識のさまざまな側面の制御を提供します。  
  
-   インプロセスで音声認識を作成するには、いずれかを使用、<xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A>コンス トラクターです。  
  
-   音声認識の文法を管理するを使用して、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A>メソッド、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A>プロパティです。  
  
-   認識エンジンへの入力を構成するのには、使用、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>、または<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>メソッドです。  
  
-   実行するには音声認識を使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>メソッドです。  
  
-   認識がサイレント状態または予期しない入力を処理する方法を変更するには、使用、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。  
  
-   認識エンジンを返しますの代替の番号を変更するを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A>プロパティです。 認識エンジンで認識の結果が返されます、<xref:System.Speech.Recognition.RecognitionResult>オブジェクト。  
  
-   認識エンジンへの変更を同期するために使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>メソッドです。 認識エンジンでは、複数のスレッドを使用して、タスクを実行します。  
  
-   認識エンジンへの入力をエミュレートするために使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッドです。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>オブジェクトが唯一は、オブジェクトをインスタンス化するプロセスを使用します。 これに対し、<xref:System.Speech.Recognition.SpeechRecognizer>がそれを使用する任意のアプリケーションで 1 つの認識を共有します。  
  
> [!NOTE]
>  常に呼び出す<xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A>音声認識エンジンへの参照を解放する前にします。 それ以外の場合、使用されているリソースは解放されませんガベージ コレクターは、認識エンジン オブジェクトのまで`Finalize`メソッドです。  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 この例を使用するため、`Multiple`のモード、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>メソッド、認識を実行には、コンソール ウィンドウを閉じるか、デバッグを停止するまでです。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> クラスの新しいインスタンスを初期化します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 構築することができます、<xref:System.Speech.Recognition.SpeechRecognitionEngine>から、次のいずれかのインスタンス。  
  
-   システムの既定の音声認識エンジン  
  
-   名前を指定している特定の音声認識エンジン  
  
-   指定したロケールの既定の音声認識エンジン  
  
-   指定した条件を満たす特定の認識エンジン、<xref:System.Speech.Recognition.RecognizerInfo>オブジェクト。  
  
 音声認識エンジンが認識を開始する前に、少なくとも 1 つの音声認識の文法を読み込むおよび認識エンジンの入力を構成する必要です。  
  
 文法を読み込むには、呼び出し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
 オーディオの入力を構成するのには、次のいずれかを使用します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>システムの既定の音声認識エンジンを使用して、<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> クラスの新しいインスタンスを初期化します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンが音声認識を開始する前に、少なくとも 1 つの認識文法を読み込むおよび認識エンジンの入力を構成する必要です。  
  
 文法を読み込むには、呼び出し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
 オーディオの入力を構成するのには、次のいずれかを使用します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">音声認識エンジンがサポートする必要があるロケール。</param>
        <summary>指定したロケールの既定の音声認識エンジンを使用して、<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> クラスの新しいインスタンスを初期化します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows および System.Speech API は、すべての有効な言語-国コードを受け取ります。 指定された言語を使用する音声認識を実行する、`CultureInfo`引数、音声認識エンジンの言語、国コードをインストールする必要がありますをサポートします。 Microsoft Windows 7 に同梱されている音声認識エンジンは、次の言語、国コードで動作します。  
  
-   en GB です。 英語 (英国)  
  
-   EN-US です。 英語 (米国)  
  
-   de DE です。 ドイツ語 (ドイツ)  
  
-   ES-ES です。 スペイン語 (スペイン)  
  
-   fr-fr などです。 フランス語 (フランス)  
  
-   JA-JP です。 日本語 (日本)  
  
-   ZH-CN です。 中国語 (中国)  
  
-   zh-tw があります。 中国語 (台湾)  
  
 "En"、"fr"などの 2 文字の言語コードまたは"es"も許可されています。  
  
 音声認識エンジンが認識を開始する前に、少なくとも 1 つの音声認識の文法を読み込むおよび認識エンジンの入力を構成する必要です。  
  
 文法を読み込むには、呼び出し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
 オーディオの入力を構成するのには、次のいずれかを使用します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示し、EN-US ロケールの音声認識エンジンを初期化するコンソール アプリケーションの一部を示します。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">インストールされた音節認識機能はいずれもこのロケールをサポートしないか、または <paramref name="culture" /> が不変のカルチャです。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> は <see langword="null" />です。</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">特定の音声認識エンジンの情報。</param>
        <summary>使用する認識エンジンを指定する <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトの情報を使用して、<see cref="T:System.Speech.Recognition.RecognizerInfo" /> クラスの新しいインスタンスを初期化します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 このクラスのインスタンスを作成するにはインストールされている音声認識機能のいずれか。 認識エンジンのインストールに関する情報を取得する、<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>メソッドです。  
  
 音声認識エンジンが認識を開始する前に、少なくとも 1 つの音声認識の文法を読み込むおよび認識エンジンの入力を構成する必要です。  
  
 文法を読み込むには、呼び出し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
 オーディオの入力を構成するのには、次のいずれかを使用します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示し、英語の言語をサポートする音声認識エンジンを初期化するコンソール アプリケーションの一部を示します。  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">使用する音声レコグナイザーのトークン名。</param>
        <summary>使用する認識エンジンの名前を指定する文字列パラメーターを使用して、<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> クラスの新しいインスタンスを初期化します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンのトークン名の値、<xref:System.Speech.Recognition.RecognizerInfo.Id%2A>のプロパティ、<xref:System.Speech.Recognition.RecognizerInfo>によって返されるオブジェクト、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A>認識エンジンのプロパティです。 インストールされているすべての認識のコレクションを取得するには、するには、静的な<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>メソッドです。  
  
 音声認識エンジンが認識を開始する前に、少なくとも 1 つの音声認識の文法を読み込むおよび認識エンジンの入力を構成する必要です。  
  
 文法を読み込むには、呼び出し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
 オーディオの入力を構成するのには、次のいずれかを使用します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 次の例は、基本的な音声認識を示し、Windows の音声認識エンジン 8.0 のインスタンスを作成するコンソール アプリケーションの一部 (英語 - 米国) です。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">そのトークン名の音声認識機能はインストールされていないか、または <paramref name="recognizerId" /> が空の文字列 ("") です。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> は <see langword="null" />です。</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> によって受け取られるオーディオの形式を取得します。</summary>
        <value>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> インスタンスへの入力時のオーディオの形式、または入力が構成されていないか null 入力に設定されている場合は <see langword="null" />。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 オーディオの入力を構成するのには、次のいずれかを使用します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 使用して次の例<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A>を取得し、オーディオ形式のデータを表示します。  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> によって受け取られるオーディオのレベルを取得します。</summary>
        <value>音声認識エンジンへの入力のオーディオ レベル (0 ～ 100)。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 値 0 は、サイレント状態を表し、100 は、最大の入力のボリュームを表します。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> がオーディオ入力のレベルを報告すると発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> 1 秒間に複数回、このイベントを発生させます。 イベントが発生する頻度は、アプリケーションが実行されているコンピューターによって異なります。  
  
 イベントの時点で、オーディオ レベルを取得する、 <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> 、関連するプロパティ<xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>です。 認識エンジンへの入力の現在のオーディオ レベルを取得するには、認識エンジンを使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A>プロパティです。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例は、ハンドラーを追加、<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>イベントを<xref:System.Speech.Recognition.SpeechRecognitionEngine>オブジェクト。 ハンドラーは、コンソールに新しいオーディオ レベルを出力します。  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> に入力を提供しているデバイスによって生成されているオーディオ ストリーム内の現在の位置を取得します。</summary>
        <value>入力デバイスによって生成されているオーディオ ストリームの現在の位置。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>プロパティは、生成されたオーディオ ストリーム内の入力デバイスの位置を参照します。 これに対し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>プロパティは、オーディオ入力内の認識エンジンの位置を参照します。 これらの位置は別にすることはできます。 たとえば、認識エンジンが受信した場合どの it されていない入力が、認識結果が次の値を生成、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>プロパティは、の値より小さい、<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>プロパティです。  
  
   
  
## Examples  
 次の例では、インプロセスで音声認識エンジンは、音声入力に一致するのに、ディクテーション文法を使用します。 ハンドラーを<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>イベントは、コンソールを書き込みます、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A>音声認識エンジンが入力に音声を検出したときにします。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> がオーディオ信号の問題を検出したときに発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 どのような問題が発生させるを使用して、 <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> 、関連するプロパティ<xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>です。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例に関する情報を収集するイベント ハンドラーを定義する、<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>イベント。  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> によって受け取られるオーディオの状態を取得します。</summary>
        <value>音声レコグナイザーへのオーディオ入力の状態。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>プロパティのメンバーでオーディオの状態を表す、<xref:System.Speech.Recognition.AudioState>列挙します。  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>オーディオの状態変化が <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> によって受け取られるときに発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 イベントの時点でオーディオの状態を取得する、 <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> 、関連するプロパティ<xref:System.Speech.Recognition.AudioStateChangedEventArgs>です。 認識エンジンへの入力の現在のオーディオ状態を取得するには、使用、認識エンジンの<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>プロパティです。 オーディオの状態に関する詳細については、次を参照してください。、<xref:System.Speech.Recognition.AudioState>列挙します。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例のハンドラーを使用して、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> 、認識エンジンを書き込むイベントの新しい<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>コンソールに変更されるたびにのメンバーを使用して、<xref:System.Speech.Recognition.AudioState>列挙します。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が認識を終了する前にバックグラウンド ノイズのみを含む入力を受け入れる時間間隔を取得または設定します。</summary>
        <value>時間の間隔。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 各音声認識エンジンには、サイレント状態と音声を区別するためのアルゴリズムがあります。 認識エンジンは、サイレント以外の状態の入力の認識エンジンのいずれかの初期のルールと一致しません背景ノイズが読み込まれ、音声認識の文法を有効になっているように分類します。 認識エンジンは、雑音タイムアウト間隔内で背景ノイズとサイレント状態のみを受信する場合、認識エンジンはその認識操作を終了します。  
  
-   非同期認識操作の場合は、認識エンジンを発生させます、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベント、場所、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType>プロパティは`true`、および<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType>プロパティは`null`します。  
  
-   同期の認識操作とエミュレーションでは、認識エンジンが返されます`null`、有効なのではなく<xref:System.Speech.Recognition.RecognitionResult>です。  
  
 雑音タイムアウト期間が 0 に設定されている場合、認識エンジンに雑音タイムアウト チェックは実行されません。 タイムアウト間隔には、任意の負でない値を指定できます。 既定値は 0 秒です。  
  
   
  
## Examples  
 次の例を設定する基本的な音声認識を示すコンソール アプリケーションの一部を示しています、<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>のプロパティ、<xref:System.Speech.Recognition.SpeechRecognitionEngine>音声認識を開始する前にします。 音声認識エンジンのハンドラー<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベントを示すために、コンソール イベント情報の出力方法、<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>のプロパティ、<xref:System.Speech.Recognition.SpeechRecognitionEngine>認識操作に影響します。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">このプロパティが 0 秒未満の値に設定されています。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトを破棄します。</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトを破棄します。</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">マネージド リソースとアンマネージド リソースの両方を解放する場合は <see langword="true" />。アンマネージド リソースだけを解放する場合は <see langword="false" />。</param>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトを破棄し、セッション中に使用するリソースを解放します。</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>同期音声認識に音声ではなくテキストを使用して、音声認識エンジンに対する入力をエミュレートします。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 これらのメソッドは、システムのオーディオ入力をバイパスし、として認識エンジンにテキストを提示<xref:System.String>オブジェクトの配列として<xref:System.Speech.Recognition.RecognizedWordUnit>オブジェクト。 これは、ことができますをテストまたはアプリケーションまたは文法をデバッグするとき。 たとえば、エミュレーションを使用すると、単語とは、文法中かどうかと、単語が認識されるときに、どのようなセマンティクスは返されるを決定します。 使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>エミュレーション操作中に音声認識エンジンにオーディオの入力を無効にする方法です。  
  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。 認識エンジンでは、新しい行と余分な空白は無視されます、リテラルの入力として区切り記号を扱います。  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult>エミュー レートされた入力に対して音声認識エンジンによって生成されたオブジェクトの値を持つ`null`の<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>プロパティです。  
  
 非同期の認識をエミュレートするために使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッドです。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">認識操作の入力。</param>
        <summary>同期音声認識に音声ではなくテキストを使用して、音声認識エンジンに対する語句の入力をエミュレートします。</summary>
        <returns>認識操作の結果。操作が不適切だったり、認識エンジンが有効でない場合は <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。  
  
 Vista および Windows 7 に付属しているレコグナイザーは、大文字小文字を区別し、入力の語句に文法ルールを適用するときに幅を文字です。 この種類の比較の詳細については、次を参照してください。、<xref:System.Globalization.CompareOptions>列挙値<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>と<xref:System.Globalization.CompareOptions.IgnoreWidth>です。 認識は、新しい行と余分な空白を無視し、リテラルの入力として句読点を扱います。  
  
   
  
## Examples  
 次のコード例は、エミュレートされた入力、関連付けられている認識の結果、および音声認識エンジンによって生成される関連付けられたイベントを示すコンソール アプリケーションの一部です。 この例には、次の出力が生成されます。  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">認識エンジンに読み込まれている音声認識文法がありません。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> が空の文字列 ("") です。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">認識操作のための必要を格納する単語単位の配列。</param>
        <param name="compareOptions">エミュレートされた認識操作に使用する比較の種類を示す列挙値のビットごとの組み合わせ。</param>
        <summary>同期音声認識にオーディオではなくテキストを使用して、音声認識エンジンに対する特定の語の入力をエミュレートし、語と読み込まれている音声認識文法との間で認識エンジンが Unicode 比較をどのように行うかを指定します。</summary>
        <returns>認識操作の結果。操作が不適切だったり、認識エンジンが有効でない場合は <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。  
  
 認識エンジンを使用して`compareOptions`ときに適用される文法ルール語句を入力します。 Vista および Windows 7 に付属しているレコグナイザーが場合は、大文字小文字を区別、<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>または<xref:System.Globalization.CompareOptions.IgnoreCase>値が存在します。 認識エンジンでは、文字幅は常に無視し、しない、ひらがなとカタカナを無視します。 認識エンジンは、新しい行と余分な空白は無視されます、リテラルの入力として区切り記号を扱います。 詳細については、文字幅、ひらがなとカタカナは、次を参照してください。、<xref:System.Globalization.CompareOptions>列挙します。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">認識エンジンに読み込まれている音声認識文法がありません。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> は 1 つ以上の <see langword="null" /> 要素を含みます。</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> は <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />、または <see cref="F:System.Globalization.CompareOptions.StringSort" /> フラグを含みます。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">認識操作の入力語句。</param>
        <param name="compareOptions">エミュレートされた認識操作に使用する比較の種類を示す列挙値のビットごとの組み合わせ。</param>
        <summary>同期音声認識にオーディオではなくテキストを使用して、音声認識エンジンに対するフレーズの入力をエミュレートし、フレーズと読み込まれている音声認識文法との間で認識エンジンが Unicode 比較をどのように行うかを指定します。</summary>
        <returns>認識操作の結果。操作が不適切だったり、認識エンジンが有効でない場合は <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。  
  
 認識エンジンを使用して`compareOptions`ときに適用される文法ルール語句を入力します。 Vista および Windows 7 に付属しているレコグナイザーが場合は、大文字小文字を区別、<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>または<xref:System.Globalization.CompareOptions.IgnoreCase>値が存在します。 認識エンジンでは、文字幅は常に無視し、しない、ひらがなとカタカナを無視します。 認識エンジンは、新しい行と余分な空白は無視されます、リテラルの入力として区切り記号を扱います。 詳細については、文字幅、ひらがなとカタカナは、次を参照してください。、<xref:System.Globalization.CompareOptions>列挙します。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">認識エンジンに読み込まれている音声認識文法がありません。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> が空の文字列 ("") です。</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> は <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />、または <see cref="F:System.Globalization.CompareOptions.StringSort" /> フラグを含みます。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>非同期音声認識に音声ではなくテキストを使用して、音声認識エンジンに対する入力をエミュレートします。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 これらのメソッドは、システムのオーディオ入力をバイパスし、として認識エンジンにテキストを提示<xref:System.String>オブジェクトの配列として<xref:System.Speech.Recognition.RecognizedWordUnit>オブジェクト。 これは、ことができますをテストまたはアプリケーションまたは文法をデバッグするとき。 たとえば、エミュレーションを使用すると、単語とは、文法中かどうかと、単語が認識されるときに、どのようなセマンティクスは返されるを決定します。 使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>エミュレーション操作中に音声認識エンジンにオーディオの入力を無効にする方法です。  
  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。 認識エンジンには、非同期の認識操作が完了すると、それを発生させる、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>イベント。 認識エンジンでは、新しい行と余分な空白は無視されます、リテラルの入力として区切り記号を扱います。  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult>エミュー レートされた入力に対して音声認識エンジンによって生成されたオブジェクトの値を持つ`null`の<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>プロパティです。  
  
 同期の認識をエミュレートするために使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>メソッドです。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">認識操作の入力。</param>
        <summary>非同期音声認識に音声ではなくテキストを使用して、音声認識エンジンに対する語句の入力をエミュレートします。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。 認識エンジンには、非同期の認識操作が完了すると、それを発生させる、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>イベント。  
  
 Vista および Windows 7 に付属しているレコグナイザーは、大文字小文字を区別し、入力の語句に文法ルールを適用するときに幅を文字です。 この種類の比較の詳細については、次を参照してください。、<xref:System.Globalization.CompareOptions>列挙値<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>と<xref:System.Globalization.CompareOptions.IgnoreWidth>です。 認識は、新しい行と余分な空白を無視し、リテラルの入力として句読点を扱います。  
  
   
  
## Examples  
 次のコード例は、非同期のエミュレートされた入力、関連付けられている認識の結果、および音声認識エンジンによって生成される関連付けられたイベントを示すコンソール アプリケーションの一部です。 この例には、次の出力が生成されます。  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">認識エンジンに、読み込まれている音声認識文法がないか、認識エンジンに、未完了の非同期認識操作が存在しています。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> が空の文字列 ("") です。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">認識操作のための必要を格納する単語単位の配列。</param>
        <param name="compareOptions">エミュレートされた認識操作に使用する比較の種類を示す列挙値のビットごとの組み合わせ。</param>
        <summary>非同期音声認識にオーディオではなく <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> の配列を使用して、共有された音声認識エンジンに対する特定の語の入力をエミュレートし、語と読み込まれている音声認識文法との間で認識エンジンが Unicode 比較をどのように行うかを指定します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。 認識エンジンには、非同期の認識操作が完了すると、それを発生させる、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>イベント。  
  
 認識エンジンを使用して`compareOptions`ときに適用される文法ルール語句を入力します。 Vista および Windows 7 に付属しているレコグナイザーが場合は、大文字小文字を区別、<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>または<xref:System.Globalization.CompareOptions.IgnoreCase>値が存在します。 認識は、常に文字幅を無視し、カナ型を無視することはありません。 認識は、新しい行と余分な空白を無視し、リテラルの入力として句読点を扱います。 詳細については、文字幅、ひらがなとカタカナは、次を参照してください。、<xref:System.Globalization.CompareOptions>列挙します。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">認識エンジンに、読み込まれている音声認識文法がないか、認識エンジンに、未完了の非同期認識操作が存在しています。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> は 1 つ以上の <see langword="null" /> 要素を含みます。</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> は <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />、または <see cref="F:System.Globalization.CompareOptions.StringSort" /> フラグを含みます。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">認識操作の入力語句。</param>
        <param name="compareOptions">エミュレートされた認識操作に使用する比較の種類を示す列挙値のビットごとの組み合わせ。</param>
        <summary>非同期音声認識にオーディオではなくテキストを使用して、音声認識エンジンに対するフレーズの入力をエミュレートし、フレーズと読み込まれている音声認識文法との間で認識エンジンが Unicode 比較をどのように行うかを指定します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンが発生し、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント認識操作がエミュレートされていない場合と同様です。 認識エンジンには、非同期の認識操作が完了すると、それを発生させる、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>イベント。  
  
 認識エンジンを使用して`compareOptions`ときに適用される文法ルール語句を入力します。 Vista および Windows 7 に付属しているレコグナイザーが場合は、大文字小文字を区別、<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>または<xref:System.Globalization.CompareOptions.IgnoreCase>値が存在します。 認識は、常に文字幅を無視し、カナ型を無視することはありません。 認識は、新しい行と余分な空白を無視し、リテラルの入力として句読点を扱います。 詳細については、文字幅、ひらがなとカタカナは、次を参照してください。、<xref:System.Globalization.CompareOptions>列挙します。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">認識エンジンに、読み込まれている音声認識文法がないか、認識エンジンに、未完了の非同期認識操作が存在しています。</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> が空の文字列 ("") です。</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> は <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />、または <see cref="F:System.Globalization.CompareOptions.StringSort" /> フラグを含みます。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> がエミュレートされた入力の非同期認識操作を終了すると発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 各<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッドが非同期認識操作を開始します。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>を生成、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>非同期操作を終了したときにイベント。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>操作を発生させることができます、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>イベントが最後にそのようなイベントが、認識エンジンは、指定した操作を発生させます。  
  
 エミュレートされた認識が成功した場合を使用して、次のいずれかの認識の結果を表示できます。  
  
-   <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A>プロパティに、<xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>のハンドラーでオブジェクト、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>イベント。  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> プロパティに、<xref:System.Speech.Recognition.SpeechRecognizedEventArgs>のハンドラーでオブジェクト、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。  
  
 エミュレートされた認識が、失敗した場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベントは発生しません、<xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A>は null になります。  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> は、<xref:System.ComponentModel.AsyncCompletedEventArgs> から派生します。  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> は、<xref:System.Speech.Recognition.RecognitionEventArgs> から派生します。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例は、音声認識の文法を読み込んで、非同期エミュー レートされた入力、関連付けられている認識の結果、音声認識エンジンによって生成される関連付けられたイベントを説明するコンソール アプリケーションの一部です。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が明確な入力の最後に認識操作を終了する前に受け入れる無音状態の間隔を取得または設定します。</summary>
        <value>無音状態の間隔の時間。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンは、認識入力があいまいな場合に、このタイムアウト間隔を使用します。 たとえば、いずれかの認識をサポートする音声認識文法"新しいゲームをしてください。"または"新しいゲーム"、"新しいゲームをしてください"、あいまいさのない入力は、"新しいゲーム"はあいまいな入力です。  
  
 このプロパティは、音声認識エンジンが待機する追加の入力を認識操作を確定する前に決定します。 タイムアウト間隔を 10 秒、包括的に 0 秒からできます。 既定値は、150 ミリ秒です。  
  
 あいまいな入力のタイムアウト間隔を設定するには、使用、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">このプロパティが、0 秒未満または 10 秒を超える値に設定されています。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が不明確な入力の最後に認識操作を終了する前に受け入れる無音状態の間隔を取得または設定します。</summary>
        <value>無音状態の間隔の時間。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンは、認識入力があいまいな場合に、このタイムアウト間隔を使用します。 たとえば、いずれかの認識をサポートする音声認識文法"新しいゲームをしてください。"または"新しいゲーム"、"新しいゲームをしてください"、あいまいさのない入力は、"新しいゲーム"はあいまいな入力です。  
  
 このプロパティは、音声認識エンジンが待機する追加の入力を認識操作を確定する前に決定します。 タイムアウト間隔を 10 秒、包括的に 0 秒からできます。 既定値は、500 ミリ秒です。  
  
 あいまいさのない入力のタイムアウト間隔を設定するには、使用、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>プロパティです。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">このプロパティが、0 秒未満または 10 秒を超える値に設定されています。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>この <see cref="T:System.Speech.Recognition.Grammar" /> インスタンスに読み込まれる <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトのコレクションを取得します。</summary>
        <value>
          <see cref="T:System.Speech.Recognition.Grammar" /> オブジェクトのコレクション。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 次の例では、音声認識エンジンによって現在読み込まれている各音声認識文法のコンソールに情報を出力します。  
  
> [!IMPORTANT]
>  このメソッドは、コレクションの要素を列挙中にコレクションが変更された場合、エラーを回避する文法コレクションをコピーします。  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が認識を終了する前に無音状態のみを含む入力を受け入れる時間間隔を取得または設定します。</summary>
        <value>無音状態の間隔の時間。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 各音声認識エンジンには、サイレント状態と音声を区別するためのアルゴリズムがあります。 認識エンジン入力が場合、サイレント状態初期サイレント状態のタイムアウト期間中に、認識エンジンはその認識操作を終了します。  
  
-   非同期認識操作とエミュレーションでは、認識エンジンを発生させます、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベント、場所、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType>プロパティは`true`、および<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType>プロパティは`null`します。  
  
-   同期の認識操作とエミュレーションでは、認識エンジンが返されます`null`、有効なのではなく<xref:System.Speech.Recognition.RecognitionResult>です。  
  
 初期のサイレント状態のタイムアウト間隔が 0 に設定されている場合、認識エンジンは、初期のサイレント状態のタイムアウトのチェックが実行されません。 タイムアウト間隔には、任意の負でない値を指定できます。 既定値は 0 秒です。  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 例のセット、<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>のプロパティ、<xref:System.Speech.Recognition.SpeechRecognitionEngine>音声認識を開始する前にします。 音声認識エンジンのハンドラー<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベントを示すために、コンソール イベント情報の出力方法、<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>のプロパティ、<xref:System.Speech.Recognition.SpeechRecognitionEngine>プロパティ認識操作に影響します。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">このプロパティが 0 秒未満の値に設定されています。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>現在のシステムにインストールされているすべての音声認識に関する情報を返します。</summary>
        <returns>インストールされたレコグナイザーを記述する <see cref="T:System.Speech.Recognition.RecognizerInfo" /> オブジェクトの読み取り専用コレクション。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 現在の認識エンジンに関する情報を取得する、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A>プロパティです。  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 例では、によって返されるコレクションを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>英語の言語をサポートする音声認識エンジンを検索するメソッド。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">読み込む文法オブジェクト。</param>
        <summary>
          <see cref="T:System.Speech.Recognition.Grammar" /> オブジェクトを同期的に読み込みます。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 場合に、認識エンジンが例外をスロー、<xref:System.Speech.Recognition.Grammar>オブジェクトは既に読み込まれて、非同期的に読み込まれる、またはがすべての認識エンジンに読み込めませんでした。 同じを読み込むことができません<xref:System.Speech.Recognition.Grammar>の複数のインスタンスにオブジェクト<xref:System.Speech.Recognition.SpeechRecognitionEngine>です。 代わりに、新しい作成<xref:System.Speech.Recognition.Grammar>ごとにオブジェクト<xref:System.Speech.Recognition.SpeechRecognitionEngine>インスタンス。  
  
 認識エンジンが実行されている場合、アプリケーションが使用する必要があります<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>を読み込み、アンロードが有効にすると、または、文法を無効にする前に、音声認識エンジンを一時停止します。  
  
 文法を読み込むときに、既定で有効です。 読み込まれた文法を無効にするを使用して、<xref:System.Speech.Recognition.Grammar.Enabled%2A>プロパティです。  
  
 読み込み、<xref:System.Speech.Recognition.Grammar>非同期的にオブジェクトを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 例は、作成、<xref:System.Speech.Recognition.DictationGrammar>音声認識エンジンに読み込みます。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> が有効な状態ではありません。</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">読み込む音声認識文法。</param>
        <summary>非同期的に音声認識文法を読み込みます。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンが読み込みが完了すると、<xref:System.Speech.Recognition.Grammar>オブジェクトを生成、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>イベント。 場合に、認識エンジンが例外をスロー、<xref:System.Speech.Recognition.Grammar>オブジェクトは既に読み込まれて、非同期的に読み込まれる、またはがすべての認識エンジンに読み込めませんでした。 同じを読み込むことができません<xref:System.Speech.Recognition.Grammar>の複数のインスタンスにオブジェクト<xref:System.Speech.Recognition.SpeechRecognitionEngine>です。 代わりに、新しい作成<xref:System.Speech.Recognition.Grammar>ごとにオブジェクト<xref:System.Speech.Recognition.SpeechRecognitionEngine>インスタンス。  
  
 認識エンジンが実行されている場合、アプリケーションが使用する必要があります<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>を読み込み、アンロードが有効にすると、または、文法を無効にする前に、音声認識エンジンを一時停止します。  
  
 文法を読み込むときに、既定で有効です。 読み込まれた文法を無効にするを使用して、<xref:System.Speech.Recognition.Grammar.Enabled%2A>プロパティです。  
  
 音声認識の文法を同期的に読み込むを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>メソッドです。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> が有効な状態ではありません。</exception>
        <exception cref="T:System.OperationCanceledException">非同期操作は取り消されました。</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が <see cref="T:System.Speech.Recognition.Grammar" /> オブジェクトの非同期読み込みを終了するときに発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンの<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドが非同期操作を開始します。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>操作が完了すると、このイベントを発生させます。 取得する、 <xref:System.Speech.Recognition.Grammar> 、認識エンジンが読み込まれているオブジェクトを使用して、 <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> 、関連するプロパティ<xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>です。 現在の取得に<xref:System.Speech.Recognition.Grammar>、認識エンジンが読み込まれたオブジェクトは、認識エンジンを使用して<xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A>プロパティです。  
  
 認識エンジンが実行されている場合、アプリケーションが使用する必要があります<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>を読み込み、アンロードが有効にすると、または、文法を無効にする前に、音声認識エンジンを一時停止します。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例では、インプロセスで音声認識エンジンを作成し、特定の単語を認識するため、空きディクテーションを受け入れるための文法の 2 種類が作成されます。 例では、構築、<xref:System.Speech.Recognition.Grammar>オブジェクトから完成した音声認識の文法の各し、非同期的に読み込みます、<xref:System.Speech.Recognition.Grammar>オブジェクトを<xref:System.Speech.Recognition.SpeechRecognitionEngine>インスタンス。 認識エンジンのハンドラー<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベントでは、名前をコンソールに書き込み、<xref:System.Speech.Recognition.Grammar>認識と認識結果のテキストをそれぞれ実行に使用されたオブジェクト。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が各認識操作に対して返す代替認識結果の最大数を取得または設定します。</summary>
        <value>返される代替結果の数。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>のプロパティ、<xref:System.Speech.Recognition.RecognitionResult>クラスのコレクションを格納する<xref:System.Speech.Recognition.RecognizedPhrase>入力の解釈を表すオブジェクト。  
  
 既定値<xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A>10 です。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">
          <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> が 0 未満の値に設定されています。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">返す設定の名前です。</param>
        <summary>レコグナイザーの設定の値を返します。</summary>
        <returns>設定値。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンの設定には、文字列、64 ビットの整数、またはメモリ アドレス データを含めることができます。 次の表に、Microsoft Speech API (SAPI) 用に定義されている設定に準拠しているレコグナイザーです。 次の設定は、同じ範囲を設定をサポートする各認識エンジンが必要です。 SAPI 準拠認識エンジンでは、これらの設定をサポートする必要はありません、その他の設定をサポートできます。  
  
|name|説明|  
|----------|-----------------|  
|`ResourceUsage`|認識エンジンの CPU 使用量を指定します。 範囲は 0 ~ 100 です。 既定値は 50 です。|  
|`ResponseSpeed`|音声認識エンジンが認識操作を完了する前に、あいまいさのない入力の最後のサイレント状態の長さを示します。 範囲は 0 から 10,000 ミリ秒 (ms) この設定は、認識エンジンに対応<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>プロパティです。  既定の 150ms を = です。|  
|`ComplexResponseSpeed`|音声認識エンジンが認識操作を完了する前に、あいまいな入力の最後のサイレント状態の長さを示します。 範囲は、0 を 10,000 ミリ秒です。 この設定は、認識エンジンに対応<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。 既定値 = 500 ミリ秒です。|  
|`AdaptationOn`|アコースティック モデルの学習機能は有効になっているかどうかを示します (値 = `1`) または OFF (値 = `0`)。 既定値は`1`(ON) です。|  
|`PersistedBackgroundAdaptation`|バック グラウンドで調整が ON であるかどうかを示す (値 = `1`) または OFF (値 = `0`)、レジストリの設定が引き続き発生するとします。 既定値は`1`(ON) です。|  
  
 更新するには、設定、認識エンジンのいずれかを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>メソッドです。  
  
   
  
## Examples  
 次の例は、EN-US ロケールをサポートする認識エンジンに定義された設定の数の値を出力するコンソール アプリケーションの一部です。 この例には、次の出力が生成されます。  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> が空の文字列 ("") です。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">認識エンジンは、その名前で設定されていません。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>同期音声認識操作を開始します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 これらのメソッドは、単一の同期の認識操作を実行します。 認識エンジンでは、そのロードされ、有効な音声認識文法に対してこの操作を実行します。  
  
 このメソッドの呼び出し中に、認識エンジンは、次のイベントを発生させることができます。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>。  認識エンジンが音声として特定することの入力を検出したときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>。  入力が、アクティブな文法のいずれかのあいまいな一致を作成するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> または <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 認識エンジンが認識操作を終了するときに発生します。  
  
 認識エンジンは発生しません、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベントのいずれかを使用する場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>メソッドです。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>を返し、<xref:System.Speech.Recognition.RecognitionResult>オブジェクト、または`null`操作が成功しなかったか、認識エンジンが有効になっていない場合。  
  
 同期認識操作は次の理由で失敗します。  
  
-   タイムアウト間隔が期限切れ前に、音声が検出されない、<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>プロパティ、または、`initialSilenceTimeout`のパラメーター、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>メソッドです。  
  
-   認識エンジンが音声を検出するが、その読み込まれ、有効になっているのいずれかに一致が検出されない<xref:System.Speech.Recognition.Grammar>オブジェクト。  
  
 修正するには、認識エンジンが音声または認識に関してサイレント状態のタイミングでどのように処理する方法を使用して、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>が少なくとも 1 台必要<xref:System.Speech.Recognition.Grammar>認識を実行する前にオブジェクトが読み込まれています。 音声認識の文法を読み込むを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
 非同期の認識を実行するのいずれかの操作を使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>メソッドです。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>同期音声認識操作を実行します。</summary>
        <returns>入力の認識結果。操作が不適切だったり、認識エンジンが有効でない場合は <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 このメソッドは、1 つの認識操作を実行します。 認識エンジンでは、そのロードされ、有効な音声認識文法に対してこの操作を実行します。  
  
 このメソッドの呼び出し中に、認識エンジンは、次のイベントを発生させることができます。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>。  認識エンジンが音声として特定することの入力を検出したときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>。  入力が、アクティブな文法のいずれかのあいまいな一致を作成するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> または <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 認識エンジンが認識操作を終了するときに発生します。  
  
 認識エンジンは発生しません、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>このメソッドを使用するときにイベント。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>メソッドを返します、<xref:System.Speech.Recognition.RecognitionResult>オブジェクト、または`null`操作が成功しなかった場合。  
  
 同期認識操作は次の理由で失敗します。  
  
-   タイムアウト間隔が期限切れ前に、音声が検出されない、<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>プロパティです。  
  
-   認識エンジンが音声を検出するが、その読み込まれ、有効になっているのいずれかに一致が検出されない<xref:System.Speech.Recognition.Grammar>オブジェクト。  
  
 非同期の認識を実行するのいずれかの操作を使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 例は、作成、<xref:System.Speech.Recognition.DictationGrammar>インプロセスで音声認識エンジンに読み込まれます、および 1 つの認識操作を実行します。  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">音声認識エンジンが認識を終了するまでに無音のみを含む入力を受け入れる時間間隔。</param>
        <summary>指定した最初のサイレント状態のタイムアウト期間の同期音声認識の操作を実行します。</summary>
        <returns>入力の認識結果。操作が不適切だったり、認識エンジンが有効でない場合は <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音声認識エンジンによって指定された時間間隔内で音声を検出`initialSilenceTimeout`引数、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29>単一認識操作を実行し、後に終了します。  `initialSilenceTimeout`パラメーターには、認識エンジンが置き換えられる<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>プロパティです。  
  
 このメソッドの呼び出し中に、認識エンジンは、次のイベントを発生させることができます。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>。  認識エンジンが音声として特定することの入力を検出したときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>。  入力が、アクティブな文法のいずれかのあいまいな一致を作成するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> または <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 認識エンジンが認識操作を終了するときに発生します。  
  
 認識エンジンは発生しません、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>このメソッドを使用するときにイベント。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>メソッドを返します、<xref:System.Speech.Recognition.RecognitionResult>オブジェクト、または`null`操作が成功しなかった場合。  
  
 同期認識操作は次の理由で失敗します。  
  
-   タイムアウト間隔が期限切れ前に、音声が検出されない、<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>または、`initialSilenceTimeout`パラメーター。  
  
-   認識エンジンが音声を検出するが、その読み込まれ、有効になっているのいずれかに一致が検出されない<xref:System.Speech.Recognition.Grammar>オブジェクト。  
  
 非同期の認識を実行するのいずれかの操作を使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 例は、作成、<xref:System.Speech.Recognition.DictationGrammar>インプロセスで音声認識エンジンに読み込まれます、および 1 つの認識操作を実行します。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>非同期音声認識操作を開始します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 これらのメソッドを行う 1 つまたは複数の非同期認識操作します。 認識エンジンでは、そのロードされ、有効な音声認識文法に対する各操作を実行します。  
  
 このメソッドの呼び出し中に、認識エンジンは、次のイベントを発生させることができます。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>。  認識エンジンが音声として特定することの入力を検出したときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>。  入力が、アクティブな文法のいずれかのあいまいな一致を作成するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> または <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 認識エンジンが認識操作を終了するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>。 いつ発生するか、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>操作が完了するとします。  
  
 非同期の認識操作の結果を取得するには、認識エンジンのため、イベント ハンドラーをアタッチします。<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。 認識エンジンでは、同期または非同期の認識操作が正常に完了するたびにこのイベントを発生させます。 認識が、失敗した場合、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>プロパティ<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>のハンドラーでアクセスできるオブジェクト、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベントになります`null`です。  
  
 次の理由で非同期認識操作を実行できます。  
  
-   タイムアウト間隔が期限切れ前に、音声が検出されない、<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>プロパティです。  
  
-   認識エンジンが音声を検出するが、その読み込まれ、有効になっているのいずれかに一致が検出されない<xref:System.Speech.Recognition.Grammar>オブジェクト。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine>が少なくとも 1 台必要<xref:System.Speech.Recognition.Grammar>認識を実行する前にオブジェクトが読み込まれています。 音声認識の文法を読み込むを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>メソッドです。  
  
-   修正するには、認識エンジンが音声または認識に関してサイレント状態のタイミングでどのように処理する方法を使用して、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。  
  
-   同期の認識を実行するのいずれかの操作を使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>メソッドです。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>単一の非同期音声認識操作を実行します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 このメソッドは、1 つ、非同期の認識操作を実行します。 認識エンジンでは、そのロードされ、有効な音声認識文法に対する操作を実行します。  
  
 このメソッドの呼び出し中に、認識エンジンは、次のイベントを発生させることができます。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>。  認識エンジンが音声として特定することの入力を検出したときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>。  入力が、アクティブな文法のいずれかのあいまいな一致を作成するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> または <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 認識エンジンが認識操作を終了するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>。 いつ発生するか、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>操作が完了するとします。  
  
 非同期の認識操作の結果を取得するには、認識エンジンのため、イベント ハンドラーをアタッチします。<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。 認識エンジンでは、同期または非同期の認識操作が正常に完了するたびにこのイベントを発生させます。 認識が、失敗した場合、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>プロパティ<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>のハンドラーでアクセスできるオブジェクト、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベントになります`null`です。  
  
 同期の認識を実行するのいずれかの操作を使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、基本的な非同期音声認識を示すコンソール アプリケーションの一部を示します。 例は、作成、<xref:System.Speech.Recognition.DictationGrammar>インプロセスで音声認識エンジンに読み込まれます、および 1 つの非同期認識操作を実行します。 イベント ハンドラーは、認識エンジンが操作中に発生するイベントを示すために含まれます。  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">1 つまたは複数の認識操作を実行するかどうかを示します。</param>
        <summary>1 つ以上の非同期音声認識操作を実行します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 場合`mode`は<xref:System.Speech.Recognition.RecognizeMode.Multiple>、認識エンジンの続行まで非同期認識操作を実行する、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>メソッドが呼び出されます。  
  
 このメソッドの呼び出し中に、認識エンジンは、次のイベントを発生させることができます。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>。  認識エンジンが音声として特定することの入力を検出したときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>。  入力が、アクティブな文法のいずれかのあいまいな一致を作成するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> または <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 認識エンジンが認識操作を終了するときに発生します。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>。 いつ発生するか、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>操作が完了するとします。  
  
 非同期の認識操作の結果を取得するには、認識エンジンのため、イベント ハンドラーをアタッチします。<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。 認識エンジンでは、同期または非同期の認識操作が正常に完了するたびにこのイベントを発生させます。 認識が、失敗した場合、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>プロパティ<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>のハンドラーでアクセスできるオブジェクト、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベントになります`null`です。  
  
 次の理由で非同期認識操作を実行できます。  
  
-   タイムアウト間隔が期限切れ前に、音声が検出されない、<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>プロパティです。  
  
-   認識エンジンが音声を検出するが、その読み込まれ、有効になっているのいずれかに一致が検出されない<xref:System.Speech.Recognition.Grammar>オブジェクト。  
  
 同期の認識を実行するのいずれかの操作を使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、基本的な非同期音声認識を示すコンソール アプリケーションの一部を示します。 例は、作成、<xref:System.Speech.Recognition.DictationGrammar>インプロセスで音声認識エンジンに読み込まれます、および複数の非同期認識操作を実行します。 非同期操作は 30 秒後に取り消されました。 イベント ハンドラーは、認識エンジンが操作中に発生するイベントを示すために含まれます。  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>現在の認識操作の完了を待たずに非同期認識を終了します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 このメソッドは、非同期の認識をすぐに終了します。 現在の非同期認識操作は、入力を受け取って場合、は、入力が切り捨てられ、既存の入力で操作が完了します。 認識エンジンが発生し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>非同期操作は取り消され、設定したときにイベント、<xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A>のプロパティ、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>に`true`です。 このメソッドによって開始される非同期操作のキャンセル、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッドです。  
  
 入力を切り捨てずに非同期の認識を停止する、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>メソッドです。  
  
   
  
## Examples  
 次の例の使用方法を示すコンソール アプリケーションの一部を示しています、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>メソッドです。 この例は、作成し、音声認識の文法を読み込みます化しが継続的非同期認識操作を開始し、操作をキャンセルする前に、2 秒間一時停止します。 認識エンジンが、ファイルから入力を受け取る c:\temp\audioinput\sample.wav です。 イベント ハンドラーは、認識エンジンが操作中に発生するイベントを示すために含まれます。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>現在の認識操作の完了後に非同期認識を停止します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 このメソッドは、入力を切り捨てずに非同期の認識を終了します。 現在の非同期認識操作は、入力を受け取って、認識エンジンが入力を受け付けると、現在認識操作が完了するまで続行されます。 認識エンジンが発生し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>非同期操作が停止し、設定したときにイベント、<xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A>のプロパティ、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>に`true`です。 このメソッドによって開始される非同期操作を停止する、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッドです。  
  
 既存の入力だけを持つ非同期の認識をすぐに取り消す場合にを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>メソッドです。  
  
   
  
## Examples  
 次の例の使用方法を示すコンソール アプリケーションの一部を示しています、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>メソッドです。 この例は、作成し、音声認識の文法を読み込みます化しが継続的非同期認識操作を開始し、操作を停止する前に、2 秒間一時停止します。 認識エンジンが、ファイルから入力を受け取る c:\temp\audioinput\sample.wav です。 イベント ハンドラーは、認識エンジンが操作中に発生するイベントを示すために含まれます。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が非同期認識操作を終了すると発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>オブジェクトの<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>メソッドが非同期認識操作を開始します。 認識エンジンには、非同期操作が終了処理として、ときにこのイベントを発生させます。  
  
 ハンドラーを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>イベントを表示する、<xref:System.Speech.Recognition.RecognitionResult>で、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>オブジェクト。 認識が、失敗した場合<xref:System.Speech.Recognition.RecognitionResult>なります`null`です。 タイムアウトまたはオーディオ入力の中断の認識が失敗する原因となったかどうかを決定するには、プロパティにアクセスすることができます<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>、 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>、または<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>です。  
  
 詳細については、<xref:System.Speech.Recognition.RecognizeCompletedEventArgs> クラスを参照してください。  
  
 拒否された認識候補として最適で詳細を取得するには、アタッチのハンドラーを<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>イベント。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例では、「ジャズ カテゴリのアーティストの一覧を表示する」や「表示アルバム考えないで」などの語句を認識します。 この例のハンドラーを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>コンソールで認識の結果に関する情報を表示するイベントです。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>処理中のオーディオ入力内の <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> の現在の位置を取得します。</summary>
        <value>処理中のオーディオ入力の認識エンジンの位置。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 オーディオの位置は、各音声認識エンジンに固有です。 入力ストリームの値 0 が有効になっているときに確立されます。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>プロパティ参照、<xref:System.Speech.Recognition.SpeechRecognitionEngine>オーディオ入力内のオブジェクトの位置。 これに対し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>プロパティは、生成されたオーディオ ストリーム内の入力デバイスの位置を参照します。 これらの位置は別にすることはできます。 たとえば、認識エンジンが受信した場合どの it されていない入力が、認識結果が次の値を生成、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>プロパティは、の値より小さい、<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>プロパティです。  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> の現在のインスタンスに関する情報を取得します。</summary>
        <value>現在の音声認識エンジンに関する情報です。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 に現在のシステムのすべてのインストール済みの音声認識機能に関する情報を取得するを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、現在のプロセスで音声認識エンジンのデータの部分的な一覧を取得します。 詳細については、「<xref:System.Speech.Recognition.RecognizerInfo>」を参照してください。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>変更を受け入れるために実行中の <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> を一時停止すると発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 アプリケーションを使用する必要があります<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>を実行中のインスタンスを一時停止する<xref:System.Speech.Recognition.SpeechRecognitionEngine>その設定を変更する前に、または<xref:System.Speech.Recognition.Grammar>オブジェクト。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>変更を受け入れる準備ができたときにこのイベントを発生させます。  
  
 などの<xref:System.Speech.Recognition.SpeechRecognitionEngine>が一時停止していることができますをロード、アンロード、有効にするには、または無効化する<xref:System.Speech.Recognition.Grammar>オブジェクトし、の値を変更、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>プロパティです。 詳細については、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> メソッドを参照してください。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例では、コンソール アプリケーションをロードおよびアンロード<xref:System.Speech.Recognition.Grammar>オブジェクト。 アプリケーションを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>音声認識エンジンの更新プログラムを受信できるようにを一時停止を要求するメソッド。 アプリケーションが読み込むか、アンロード、<xref:System.Speech.Recognition.Grammar>オブジェクト。  
  
 各更新プログラムのハンドラーで<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベントが現在読み込み済みの状態と名前を書き込みます<xref:System.Speech.Recognition.Grammar>オブジェクトをコンソールです。 ように文法では、読み込まれ、アンロードすることが、アプリケーションは、ファーム動物の名前、果物の名前およびファーム動物の名前にし果物の名前だけに最初認識します。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>認識エンジンが状態の更新を停止することを要求します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンへの変更を同期するために、このメソッドを使用します。 たとえば、読み込み、認識エンジンが入力の処理中に音声認識の文法をアンロードするか、このメソッドを使用し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>認識エンジンの状態とアプリケーションの動作を同期するイベントです。  
  
 このメソッドが呼び出されると、一時停止または非同期操作が完了して、レコグナイザーが生成されます、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント。 A<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント ハンドラーは認識操作の間、認識エンジンの状態を変更できます。 処理するときに<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント、イベント ハンドラーを返すまで、認識エンジンが一時停止します。  
  
> [!NOTE]
>  認識エンジンが発生する前に、認識エンジンへの入力が変更された場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント、要求が破棄されます。  
  
 このメソッドが呼び出されたとき。  
  
-   認識エンジンが直ちに生成、認識エンジンが入力を処理されていない場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント。  
  
-   認識エンジンが認識操作を一時停止し、生成、認識エンジンがサイレント状態のバック グラウンド ノイズで構成される入力を処理している場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント。  
  
-   認識エンジンが認識操作を完了し、生成、認識エンジンがサイレント状態またはバック グラウンド ノイズ数に達していない入力を処理している場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント。  
  
 認識エンジンが処理中に、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント。  
  
-   認識エンジンでは、入力、およびの値は処理されません、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>プロパティは変わりません。  
  
-   認識エンジンを継続して入力を収集しの値、<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>プロパティを変更することができます。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>認識エンジンが状態の更新を停止することを要求します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンが生成するとき、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント、<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>のプロパティ、<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs>は`null`します。  
  
 ユーザー トークンを指定するには、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>メソッドです。 オーディオの位置のオフセットを指定するには、使用、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、コンソール アプリケーションをロードおよびアンロード<xref:System.Speech.Recognition.Grammar>オブジェクト。 アプリケーションを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>音声認識エンジンの更新プログラムを受信できるようにを一時停止を要求するメソッド。 アプリケーションが読み込むか、アンロード、<xref:System.Speech.Recognition.Grammar>オブジェクト。  
  
 各更新プログラムのハンドラーで<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベントが現在読み込み済みの状態と名前を書き込みます<xref:System.Speech.Recognition.Grammar>オブジェクトをコンソールです。 ように文法では、読み込まれ、アンロードすることが、アプリケーションは、ファーム動物の名前、果物の名前およびファーム動物の名前にし果物の名前だけに最初認識します。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">操作に関する情報を含むユーザー定義情報。</param>
        <summary>認識エンジンが状態の更新を停止し、関連イベントのユーザー トークンを提供することを要求します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンが生成するとき、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント、<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>のプロパティ、<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs>の値が含まれています、`userToken`パラメーター。  
  
 オーディオの位置のオフセットを指定するには、使用、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>メソッドです。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">操作に関する情報を含むユーザー定義情報。</param>
        <param name="audioPositionAheadToRaiseUpdate">要求を遅延するための、現在の <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> からのオフセット。</param>
        <summary>認識エンジンが状態の更新を停止し、関連イベントのオフセットとユーザー トークンを提供することを要求します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンが認識エンジンのまで認識エンジンの更新要求を開始できません<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>現在に等しい<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>plus`audioPositionAheadToRaiseUpdate`です。  
  
 認識エンジンが生成するとき、<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>イベント、<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>のプロパティ、<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs>の値が含まれています、`userToken`パラメーター。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">オーディオ入力ストリーム。</param>
        <param name="audioFormat">オーディオ入力の形式。</param>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトを、オーディオ ストリームからの入力を受け取るように構成します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンでは、認識操作中に、入力ストリームの末尾に達すると、使用可能な入力の認識操作を終了します。 認識エンジンへの入力を更新する場合を除きの後で認識操作は、例外を生成できます。  
  
   
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 例では、入力を使用して、オーディオ ファイル、フレーズを含む、example.wav から"1 つのテストのテスト 2 3"と"先々 cooper"、一時停止で区切られました。 この例には、次の出力が生成されます。  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトを、既定のオーディオ デバイスからの入力を受け取るように構成します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 次の例では、基本的な音声認識を示すコンソール アプリケーションの一部を示します。 例では、既定のオーディオ デバイスからの出力を使用して、複数の実行、非同期の認識操作と終了します。 ユーザーは、語句を utters ときに"exit"です。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>音声レコグナイザーに対する入力を無効にします。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 構成、<xref:System.Speech.Recognition.SpeechRecognitionEngine>オブジェクトの入力を使用する場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>と<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッド、または一時的にオフライン認識エンジンを作成する場合。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">入力として使用するファイルのパス。</param>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトを、WAVE オーディオ形式 (.wav) ファイルからの入力を受け取るように構成します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンでは、認識操作中に、入力ファイルの末尾に達すると、使用可能な入力の認識操作を終了します。 認識エンジンへの入力を更新する場合を除きの後で認識操作は、例外を生成できます。  
  
   
  
## Examples  
 次の例では、.wav ファイルにオーディオの認識を実行し、コンソールに、認識されたテキストを書き込みます。  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">オーディオ データが含まれるストリーム。</param>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトを、WAVE オーディオ形式 (.wav) データを含むストリームからの入力を受け取るように構成します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンでは、認識操作中に、入力ストリームの末尾に達すると、使用可能な入力の認識操作を終了します。 認識エンジンへの入力を更新する場合を除きの後で認識操作は、例外を生成できます。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が音声として識別できる入力を検出すると発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 各音声認識エンジンには、サイレント状態と音声を区別するためのアルゴリズムがあります。 ときに、<xref:System.Speech.Recognition.SpeechRecognitionEngine>音声認識操作を実行を発生させる、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>のアルゴリズムは、音声入力として入力を識別するときにイベント。 <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> 、関連するプロパティ<xref:System.Speech.Recognition.SpeechDetectedEventArgs>オブジェクトを認識エンジンが音声を認識する場合、入力ストリーム内の場所を示します。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>を生成、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>イベントのいずれかが発生する前に、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>、または<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>イベント。  
  
 詳細については、次を参照してください。、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッドです。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例は、フライトの送信元と送信先の都市を選択するためのコンソール アプリケーションの一部です。 アプリケーションが語句を認識する Miami からシカゴにスライドします"など  この例では、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>イベント レポートを<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>各時間音声が検出されました。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が文法の複数の完全な語句のコンポーネントである可能性がある単語を認識した場合に発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>多数生成<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>イベントとして、入力の語句を識別しようとしました。 部分的に認識された語句を指定のテキストにアクセスすることができます、<xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>のプロパティ、<xref:System.Speech.Recognition.SpeechHypothesizedEventArgs>のハンドラーでオブジェクト、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>イベント。 通常、これらのイベントの処理は、のみのデバッグに役立ちます。  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> は、<xref:System.Speech.Recognition.RecognitionEventArgs> から派生します。  
  
 詳細については、次を参照してください。、<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティおよび<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>メソッドです。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例では、「ジャズのカテゴリのアーティストの一覧を表示する」などの語句を認識します。 この例では、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>認識するように、コンソールで不完全な語句フラグメントを表示するイベントです。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が読み込み済みで有効な <see cref="T:System.Speech.Recognition.Grammar" /> オブジェクトのどのリストにも一致しない入力を受け取ると、発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 入力が一致しないことための十分な信頼度がすべての読み込まれたと有効になっていると判断した場合、認識エンジンにこのイベントを発生させます<xref:System.Speech.Recognition.Grammar>オブジェクト。 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>のプロパティ、 <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> 、拒否されたを含む<xref:System.Speech.Recognition.RecognitionResult>オブジェクト。 ハンドラーを使用することができます、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>認識を取得するイベント<xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>が却下されたことと、それら<xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A>スコア。  
  
 アプリケーションが使用されている場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine>インスタンス、どの音声で入力が受理されても拒否のいずれかの信頼レベルを変更することができます、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>メソッドです。 音声認識が以外の音声を使用して入力に応答する方法を変更することができます、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例では、「ジャズ カテゴリのアーティストの一覧を表示する」や「表示アルバム考えないで」などの語句を認識します。 例では、ハンドラーを使用して、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 、音声入力が十分である文法の内容に一致することはできません、コンソールの通知を表示するイベント<xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A>成功認識を生成するためにします。 ハンドラーでは、認識結果も表示されます<xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>低信頼スコアにより拒否されたことです。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> が読み込み済みで有効な <see cref="T:System.Speech.Recognition.Grammar" /> オブジェクトのいずれかのリストに一致する入力を受け取ると、発生します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 いずれかを使用して認識操作を開始することができます、<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>または<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>メソッドです。 認識エンジンが発生し、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>入力に、そのロードのいずれかと一致すると判断した場合、イベント<xref:System.Speech.Recognition.Grammar>認識を構成する信頼のための十分なレベルのオブジェクト。 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>のプロパティ、<xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs>承諾が含まれています<xref:System.Speech.Recognition.RecognitionResult>オブジェクト。 ハンドラー<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベントは、認識された語句と認識機能の一覧を取得できます<xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>低い信頼度スコアを持つ。  
  
 アプリケーションが使用されている場合、<xref:System.Speech.Recognition.SpeechRecognitionEngine>インスタンス、どの音声で入力が受理されても拒否のいずれかの信頼レベルを変更することができます、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>メソッドです。  音声認識が以外の音声を使用して入力に応答する方法を変更することができます、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。  
  
 認識エンジンが、文法に一致する入力を受け取るときに、<xref:System.Speech.Recognition.Grammar>オブジェクトを発生させることができます、<xref:System.Speech.Recognition.Grammar.SpeechRecognized>イベント。 <xref:System.Speech.Recognition.Grammar>オブジェクトの<xref:System.Speech.Recognition.Grammar.SpeechRecognized>音声認識エンジンの前にイベントが発生した<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。 特定の文法に固有のすべてのタスクは、のハンドラーによって常に実行する必要があります、<xref:System.Speech.Recognition.Grammar.SpeechRecognized>イベント。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> デリゲートを作成する場合は、イベントを処理するメソッドを指定します。 イベントをイベント ハンドラーに関連付けるには、デリゲートのインスタンスをイベントに追加します。 デリゲートを削除しない限り、そのイベントが発生すると常にイベント ハンドラーが呼び出されます。 イベント ハンドラー デリゲートの詳細については、次を参照してください。[イベントとデリゲート](http://go.microsoft.com/fwlink/?LinkId=162418)です。  
  
   
  
## Examples  
 次の例は、音声認識の文法、構造を作成するコンソール アプリケーションの一部、<xref:System.Speech.Recognition.Grammar>オブジェクト、および、読み込まれるときに、<xref:System.Speech.Recognition.SpeechRecognitionEngine>認識を実行します。 例は、音声入力、 <xref:System.Speech.Recognition.SpeechRecognitionEngine>、関連付けられている認識結果、および音声認識エンジンによって生成される関連するイベントです。  
  
 「する Miami にシカゴからスライド」をトリガーするように入力を読み上げ、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。 "飛行 me ヒューストンからシカゴに"という語句を話すはトリガーされません、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>イベント。  
  
 例では、ハンドラーを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>を正常に表示するイベントには、語句と、コンソールに含まれているセマンティクスが認識されています。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>すべての <see cref="T:System.Speech.Recognition.Grammar" /> オブジェクトを認識エンジンから削除します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンが現在を読み込んでいる場合、<xref:System.Speech.Recognition.Grammar>までこのメソッドは、非同期的に待機、<xref:System.Speech.Recognition.Grammar>読み込まれると、すべてのアンロードする前に、<xref:System.Speech.Recognition.Grammar>オブジェクトから、<xref:System.Speech.Recognition.SpeechRecognitionEngine>インスタンス。  
  
 特定の文法をアンロードするを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、同期の読み込みとアンロード音声認識の文法を示すコンソール アプリケーションの一部を示します。  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">アンロードする文法オブジェクト。</param>
        <summary>
          <see cref="T:System.Speech.Recognition.Grammar" /> インスタンスから、指定された <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> オブジェクトをアンロードします。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンが実行されている場合、アプリケーションが使用する必要があります<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>を一時停止、<xref:System.Speech.Recognition.SpeechRecognitionEngine>読み込み、アンロードが有効にすると、または無効にする前にインスタンス、<xref:System.Speech.Recognition.Grammar>オブジェクト。 すべてをアンロードする<xref:System.Speech.Recognition.Grammar>オブジェクトを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A>メソッドです。  
  
   
  
## Examples  
 次の例では、同期の読み込みとアンロード音声認識の文法を示すコンソール アプリケーションの一部を示します。  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.InvalidOperationException">文法は、この認識エンジンに読み込まれていないか、この認識エンジンは現在文法を非同期に読み込んでいます。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>認識エンジンの設定の値を更新します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 認識エンジンの設定には、文字列、64 ビットの整数、またはメモリ アドレス データを含めることができます。 次の表に、Microsoft Speech API (SAPI) 用に定義されている設定に準拠しているレコグナイザーです。 次の設定は、同じ範囲を設定をサポートする各認識エンジンが必要です。 SAPI 準拠認識エンジンでは、これらの設定をサポートする必要はありません、その他の設定をサポートできます。  
  
|name|説明|  
|----------|-----------------|  
|`ResourceUsage`|認識エンジンの CPU 使用量を指定します。 範囲は 0 ~ 100 です。 既定値は 50 です。|  
|`ResponseSpeed`|音声認識エンジンが認識操作を完了する前に、あいまいさのない入力の最後のサイレント状態の長さを示します。 範囲は 0 から 10,000 ミリ秒 (ms) この設定は、認識エンジンに対応<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>プロパティです。 既定の 150ms を = です。|  
|`ComplexResponseSpeed`|音声認識エンジンが認識操作を完了する前に、あいまいな入力の終わりをミリ秒 (ms) のサイレント状態の長さを示します。 範囲は、0 を 10,000 ミリ秒です。 この設定は、認識エンジンに対応<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。 既定値 = 500 ミリ秒です。|  
|`AdaptationOn`|アコースティック モデルの学習機能は有効になっているかどうかを示します (値 = `1`) または OFF (値 = `0`)。 既定値は`1`(ON) です。|  
|`PersistedBackgroundAdaptation`|バック グラウンドで調整が ON であるかどうかを示す (値 = `1`) または OFF (値 = `0`)、レジストリの設定が引き続き発生するとします。 既定値は`1`(ON) です。|  
  
 認識エンジンの設定のいずれかを返すを使用して、<xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A>メソッドです。  
  
 例外を除いて`PersistedBackgroundAdaptation`、プロパティの値を使用して設定、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>メソッドは、の現在のインスタンスに対してのみ有効になります<xref:System.Speech.Recognition.SpeechRecognitionEngine>既定の設定に戻すそれらを後にします。  
  
 音声認識が以外の音声を使用して入力に応答する方法を変更することができます、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>、 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>、および<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>プロパティです。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">更新する設定の名前。</param>
        <param name="updatedValue">設定の新しい値。</param>
        <summary>
          <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> の指定された設定を、指定された整数値で更新します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 例外を除いて`PersistedBackgroundAdaptation`、プロパティの値を使用して設定、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>メソッドは、の現在のインスタンスに対してのみ有効になります<xref:System.Speech.Recognition.SpeechRecognitionEngine>既定の設定に戻すそれらを後にします。 参照してください<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>サポートされている設定の詳細についてはします。  
  
   
  
## Examples  
 次の例は、EN-US ロケールをサポートする認識エンジンに定義された設定の数の値を出力するコンソール アプリケーションの一部です。 この例では、信頼レベルの設定を更新し、更新された値をチェックする認識エンジンのクエリを発行しています。 この例には、次の出力が生成されます。  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> が空の文字列 ("") です。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">認識エンジンは、その名前で設定されていません。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">更新する設定の名前。</param>
        <param name="updatedValue">設定の新しい値。</param>
        <summary>指定された音声認識エンジンの設定を、指定された文字列値で更新します。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 例外を除いて`PersistedBackgroundAdaptation`、プロパティの値を使用して設定、<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>メソッドは、の現在のインスタンスに対してのみ有効になります<xref:System.Speech.Recognition.SpeechRecognitionEngine>既定の設定に戻すそれらを後にします。 参照してください<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>サポートされている設定の詳細についてはします。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> は <see langword="null" />です。</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> が空の文字列 ("") です。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">認識エンジンは、その名前で設定されていません。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>